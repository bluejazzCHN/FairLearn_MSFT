{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Unfairness in the Law School Dataset\n",
    "\n",
    "In this example, we will examine the well known Law School Admissions dataset, provided by [Project SEAPHE](http://www.seaphe.org/databases.php). The motivation was to gain a better understanding of race in law school admissions, and ensuring that students who would ultimately pass the bar exam were treated fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We obtain the data from the `tempeh` package. The main feature data (which we will refer to as $X$) has two features - undergraduate GPA and LSAT score. The label (which we call $y$) is 0 or 1 dependent on whether that student passed the bar exam. Finally, we also have the race of the students ('black' or white') as a sensitive attribute, which we will refer to as $A$.\n",
    "\n",
    "We start by loading the data, which have already been split into \"train\" and \"test\" subsets for us. However, we do need to rescale the two features in $X$ to lie in the range $[0, 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(dataset.X_train), columns=dataset.features)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(dataset.X_test), columns=dataset.features)\n",
    "\n",
    "y_train = pd.Series(dataset.y_train.squeeze(), name=\"Pass Bar\")\n",
    "y_test = pd.Series(dataset.y_test.squeeze(), name=\"Pass Bar\")\n",
    "\n",
    "A_train = pd.Series(dataset.race_train, name=\"Race\")\n",
    "A_test = pd.Series(dataset.race_test, name=\"Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us examine the data. First, we can look at the breakdown of students by race in the dataset. We see that there are far more white students than black, which is already a suggestion of bias in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, c = np.unique(dataset.race_train, return_counts=True)\n",
    "for i in range(len(l)):\n",
    "    print(\"Number of {0} students is {1}\".format(l[i], c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start using the group metrics from `fairlearn` to examine things such as the final pass rate for the bar exam. Both rates are high, although higher for whites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_mean_prediction\n",
    "\n",
    "def group_metric_printer(name, group_metric_result):\n",
    "    print(\"{0} overall {1:.3f}\".format(name, group_metric_result.overall))\n",
    "    for k, v in group_metric_result.by_group.items():\n",
    "        print(\"{0} for {1:8} {2:.3f}\".format(name, k, v))\n",
    "\n",
    "unused = np.ones(len(dataset.y_train))\n",
    "group_metric_printer(\"Pass Rate\", group_mean_prediction(unused, y_train, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the [ROC-AUC scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for each feature (that is LSAT score and undergraduate GPA) for the overall dataset and by race. Used in this way, the ROC-AUC score is a measure of how predictive each feature is of the final label. A score of 0.5 would mean that the feature is no better than a coin flip (a coin biased to produce a desired fraction of positives), while a score of 1 means that the feature is perfectly discriminating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score\n",
    "\n",
    "for column_name in X_train:\n",
    "    column_data = X_train[column_name]\n",
    "    title = \"ROC-AUC {0}\".format(column_name)\n",
    "    group_metric_printer(title, group_roc_auc_score(y_train, column_data, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the CDFs for the LSAT and GPAs for whites and blacks (recall that we rescaled both to $[0,1]$ above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import cumfreq\n",
    "\n",
    "def plot_separated_cdf(data, A):\n",
    "    for a in np.unique(A):\n",
    "        subset = data[A==a]\n",
    "        \n",
    "        cdf = cumfreq(subset, numbins=20)\n",
    "        x = cdf.lowerlimit + np.linspace(0, cdf.binsize*cdf.cumcount.size, cdf.cumcount.size)\n",
    "        plt.plot(x, cdf.cumcount / len(subset), label=a)\n",
    "    plt.xlabel(data.name)\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "plot_separated_cdf(X_train['lsat'], A_train)\n",
    "plot_separated_cdf(X_train['ugpa'], A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unmitigated Predictor\n",
    "\n",
    "As a point of comparison for later, we can train a predictor without regard to fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this predictor, we can look at some statistics. First, we can look at the average predictions. Immediately we see that a 100% pass rate for whites is predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_mean_predictions = group_mean_prediction(y_test, # Actually unused\n",
    "                                                     unmitigated_predictor.predict(X_test),\n",
    "                                                     A_test)\n",
    "group_metric_printer(\"Predicted Pass Rate\", unmitigated_mean_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we need to think a little more deeply about what we're doing. Ultimately, we would want to use this model for admissions, and we would want to admit students (fairly) according to their chances of passing the bar exam. The `LogisticRegression` estimator provides a `predict_proba` method for this purpose - this provides the probability of predicting a given class, which is then thresholded by the `predict` method itself.\n",
    "\n",
    "First, we can obtain the appropriate probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unmitigated = pd.Series(unmitigated_predictor.predict_proba(X_test)[:,1], name=\"Pass Probability Unmitigated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the mean predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric_printer(\"Predicted Pass Probability\", group_mean_prediction(y_test, y_pred_unmitigated, A_test))\n",
    "plot_separated_cdf(y_pred_unmitigated, A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the ROC-AUC scores for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_roc_auc_score_unmitigated = group_roc_auc_score(y_test, y_pred_unmitigated, A_test)\n",
    "group_metric_printer(\"Unmitigated ROC-AUC score\", group_roc_auc_score_unmitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfairness Mitigation with Grid Search\n",
    "\n",
    "In this section, we will attempt to mitigate the unfairness in the incoming data using the `GridSearch` algorithm of `fairlearn`. We shall apply constraints of demographic parity - that is, we will attempt to equalise the positive prediction rates between whites and blacks. This is appropriate for affirmative action scenarios.\n",
    "\n",
    "We do a grid search in two stages. In the first, we do a low resolution search, with `fairlearn` chosing the grid for us. From this, we identify a region to expand the grid, and do a more detailed sweep around that point.\n",
    "\n",
    "First, the low resolution sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, DemographicParity\n",
    "\n",
    "n_sweep = 9\n",
    "\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=n_sweep)\n",
    "\n",
    "sweep.fit(X_train, y_train, sensitive_features=A_train)\n",
    "\n",
    "print(sweep.best_result.lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best $\\lambda$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec_best = sweep.best_result.lambda_vec\n",
    "lambda_best = lambda_vec_best[(\"+\", \"all\", \"white\")] - lambda_vec_best[(\"-\", \"all\", \"white\")]\n",
    "print(\"lambda_best =\", lambda_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a higher resolution grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_second_sweep = 51\n",
    "second_sweep_multipliers = np.linspace(lambda_best-0.5, lambda_best+0.5, n_second_sweep)\n",
    "\n",
    "iterables = [['+','-'], ['all'], ['black', 'white']]\n",
    "midx = pd.MultiIndex.from_product(iterables, names=['sign', 'event', 'group_id'])\n",
    "\n",
    "second_sweep_lambdas = []\n",
    "for l in second_sweep_multipliers:\n",
    "    nxt = pd.Series(np.zeros(4), index=midx)\n",
    "    if l < 0:\n",
    "        nxt[(\"-\", \"all\", \"white\")] = abs(l)\n",
    "    else:\n",
    "        nxt[(\"+\", \"all\", \"white\")] = l\n",
    "    second_sweep_lambdas.append(nxt)\n",
    "    \n",
    "multiplier_df = pd.concat(second_sweep_lambdas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the new search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid=multiplier_df)\n",
    "\n",
    "second_sweep.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our set of models, we can do some analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Grid Search results\n",
    "\n",
    "We have used the `GridSearch` algorithm with demographic parity constraints. However, as noted above, for analysing our results, it is more appropriate to look at the predicted probabilities and the ROC-AUC scores.\n",
    "\n",
    "We can plot these scores against the mean disparity (between blacks and whites) for each model. We can see that for very little change in the worst ROC-AUC score, we can substantially reduce the disparity, as measured by the mean prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_roc_auc_score = np.zeros(n_second_sweep)\n",
    "sweep_mean_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(X_test)[:,1]\n",
    "    sweep_roc_auc_score[i] = group_roc_auc_score(y_test, preds, A_test).minimum\n",
    "    sweep_mean_disparity[i] = group_mean_prediction(y_test, preds, A_test).range\n",
    "    \n",
    "plt.scatter(sweep_roc_auc_score, sweep_mean_disparity)\n",
    "plt.xlabel(\"Minimum ROC AUC Score\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how the predictions are varying with the set of generated Lagrange multipliers. What we see is that we are gradually moving to predict that all students pass the bar. The 'opportunity gap' between the two sets of points matches the range in disparities in the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = [group_mean_prediction(y_test, x.predictor.predict_proba(X_test)[:,1], A_test)\n",
    "                   for x in second_sweep.all_results]\n",
    "\n",
    "for r in ['black', 'white']:\n",
    "    plt.scatter(second_sweep_multipliers, [x.by_group[r] for x in mean_predictions], label=r)\n",
    "plt.xlabel(\"Multiplier\")\n",
    "plt.ylabel(\"Opportunity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
