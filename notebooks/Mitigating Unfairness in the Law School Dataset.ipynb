{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Unfairness in the Law School Dataset\n",
    "\n",
    "In this example, we will examine the well known Law School Admissions dataset, provided by [Project SEAPHE](http://www.seaphe.org/databases.php). The motivation was to gain a better understanding of race in law school admissions, and ensuring that students who would ultimately pass the bar exam were treated fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We obtain the data from the `tempeh` package. The main feature data (which we will refer to as $X$) has two features - undergraduate GPA and LSAT score. The label (which we call $y$) is 0 or 1 dependent on whether that student passed the bar exam. Finally, we also have the race of the students ('black' or white') as a sensitive attribute, which we will refer to as $A$.\n",
    "\n",
    "We start by loading the data, which have already been split into \"train\" and \"test\" subsets for us. However, we do need to rescale the two features in $X$ to lie in the range $[0, 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(dataset.X_train), columns=dataset.features)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(dataset.X_test), columns=dataset.features)\n",
    "\n",
    "y_train = pd.Series(dataset.y_train.squeeze(), name=\"Pass Bar\", dtype=int)\n",
    "y_test = pd.Series(dataset.y_test.squeeze(), name=\"Pass Bar\", dtype=int)\n",
    "\n",
    "A_train = pd.Series(dataset.race_train, name=\"Race\")\n",
    "A_test = pd.Series(dataset.race_test, name=\"Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us examine the data. First, we can look at the breakdown of students by race in the dataset. We see that there are far more white students than black, which is already a suggestion of bias in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, c = np.unique(dataset.race_train, return_counts=True)\n",
    "for i in range(len(l)):\n",
    "    print(\"Number of {0} students is {1}\".format(l[i], c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start using the group metrics from `fairlearn` to examine things such as the final pass rate for the bar exam. Both rates are high, although higher for whites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_mean_prediction\n",
    "\n",
    "def group_metric_printer(name, group_metric_result):\n",
    "    print(\"{0} overall {1:.3f}\".format(name, group_metric_result.overall))\n",
    "    for k, v in group_metric_result.by_group.items():\n",
    "        print(\"{0} for {1:8} {2:.3f}\".format(name, k, v))\n",
    "\n",
    "unused = np.ones(len(dataset.y_train))\n",
    "group_metric_printer(\"Pass Rate\", group_mean_prediction(unused, y_train, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw numbers, we see how dominant whites who pass the bar exam are in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in ['black', 'white']:\n",
    "    Ys = y_train[A_train==r]\n",
    "    print(r)\n",
    "    print(Ys.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the [ROC-AUC scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for each feature (that is LSAT score and undergraduate GPA) for the overall dataset and by race. Used in this way, the ROC-AUC score is a measure of how predictive each feature is of the final label. A score of 0.5 would mean that the feature is no better than a coin flip (a coin biased to produce a desired fraction of positives), while a score of 1 means that the feature is perfectly discriminating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score\n",
    "\n",
    "for column_name in X_train:\n",
    "    column_data = X_train[column_name]\n",
    "    title = \"ROC-AUC {0}\".format(column_name)\n",
    "    group_metric_printer(title, group_roc_auc_score(y_train, column_data, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the CDFs for the LSAT and GPAs for whites and blacks (recall that we rescaled both to $[0,1]$ above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import cumfreq\n",
    "\n",
    "def plot_separated_cdf(data, A):\n",
    "    for a in np.unique(A):\n",
    "        subset = data[A==a]\n",
    "        \n",
    "        cdf = cumfreq(subset, numbins=20)\n",
    "        x = cdf.lowerlimit + np.linspace(0, cdf.binsize*cdf.cumcount.size, cdf.cumcount.size)\n",
    "        plt.plot(x, cdf.cumcount / len(subset), label=a)\n",
    "    plt.xlabel(data.name)\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "plot_separated_cdf(X_train['lsat'], A_train)\n",
    "plot_separated_cdf(X_train['ugpa'], A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unmitigated Predictor\n",
    "\n",
    "As a point of comparison for later, we can train a predictor without regard to fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this predictor, we can look at some statistics. First, we can look at the average predictions. Immediately we see that a 100% pass rate for whites is predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_mean_predictions = group_mean_prediction(y_test, # Actually unused\n",
    "                                                     unmitigated_predictor.predict(X_test),\n",
    "                                                     A_test)\n",
    "group_metric_printer(\"Predicted Pass Rate\", unmitigated_mean_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we need to think a little more deeply about what we're doing. Ultimately, we would want to use this model for admissions, and we would want to admit students (fairly) according to their chances of passing the bar exam. The `LogisticRegression` estimator provides a `predict_proba` method for this purpose - this provides the probability of predicting a given class, which is then thresholded by the `predict` method itself.\n",
    "\n",
    "First, we can obtain the appropriate probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unmitigated = pd.Series(unmitigated_predictor.predict_proba(X_test)[:,1], name=\"Pass Probability Unmitigated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the mean predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric_printer(\"Predicted Pass Probability\", group_mean_prediction(y_test, y_pred_unmitigated, A_test))\n",
    "plot_separated_cdf(y_pred_unmitigated, A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the ROC-AUC scores for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_roc_auc_score_unmitigated = group_roc_auc_score(y_test, y_pred_unmitigated, A_test)\n",
    "group_metric_printer(\"Unmitigated ROC-AUC score\", group_roc_auc_score_unmitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfairness Mitigation with Grid Search\n",
    "\n",
    "In this section, we will attempt to mitigate the unfairness in the incoming data using the `GridSearch` algorithm of `fairlearn`. We shall apply constraints of demographic parity - that is, we will attempt to equalise the positive prediction rates between whites and blacks. This is appropriate for affirmative action scenarios.\n",
    "\n",
    "We do a grid search in two stages. In the first, we do a low resolution search, identifying a region of promising looking Lagrange multipliers. We will then perform a higher resolution search around that region.\n",
    "\n",
    "Due to the extreme imbalance in the data, we can't use the default grid for the initial sweep, but must specify one ourselves. We will use the following subroutine for grid generation, which transforms an array of multiplier values into the form required by `GridSearch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_generator(multipliers):\n",
    "    iterables = [['+','-'], ['all'], ['black', 'white']]\n",
    "    midx = pd.MultiIndex.from_product(iterables, names=['sign', 'event', 'group_id'])\n",
    "    \n",
    "    sweep_lambdas = []\n",
    "    for l in multipliers:\n",
    "        nxt = pd.Series(np.zeros(4), index=midx)\n",
    "        if l < 0:\n",
    "            nxt[(\"-\", \"all\", \"white\")] = abs(l)\n",
    "        else:\n",
    "            nxt[(\"+\", \"all\", \"white\")] = l\n",
    "        sweep_lambdas.append(nxt)\n",
    "            \n",
    "    return pd.concat(sweep_lambdas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the low resolution sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, DemographicParity\n",
    "\n",
    "n_first_sweep = 21\n",
    "first_multipliers = np.linspace(-10, 10, n_first_sweep)\n",
    "\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid=grid_generator(first_multipliers))\n",
    "\n",
    "sweep.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the mean opportunity of these models as a function of the multiplier used. We can also see that the opportunity is equalised for blacks and whites with a multiplier of around six."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_sweep_plot(multipliers, all_results, metric_func):\n",
    "    assert len(multipliers)==len(all_results)\n",
    "    metrics = [metric_func(y_test, x.predictor.predict_proba(X_test)[:,1], A_test)\n",
    "               for x in all_results]\n",
    "    \n",
    "    for r in ['black', 'white']:\n",
    "        plt.scatter(multipliers, [x.by_group[r] for x in metrics], label=r)\n",
    "    plt.scatter(multipliers, [x.overall for x in metrics], label='overall')\n",
    "    plt.xlabel(\"Multiplier\")\n",
    "    plt.ylabel(metric_func.__name__[6:])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "metric_sweep_plot(first_multipliers, sweep.all_results, metric_func=group_mean_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the ROC-AUC score for this set of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_sweep_plot(first_multipliers, sweep.all_results, metric_func=group_roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the disparity in the opportunity with the ROC-AUC score for each model. This will give a better idea of the tradeoffs available to us. This shows that we can substantially cut the disparity in opportunity with a minimal effect on the ROC-AUC score. In the second sweep, we shall seek to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc_disparity_sweep_plot(multipliers, all_results):\n",
    "    assert len(multipliers)==len(all_results)\n",
    "    \n",
    "    roc_auc = np.zeros(len(multipliers))\n",
    "    disparity = np.zeros(len(multipliers))\n",
    "    \n",
    "    for i in range(len(multipliers)):\n",
    "        preds = all_results[i].predictor.predict_proba(X_test)[:,1]\n",
    "        roc_auc[i] = group_roc_auc_score(y_test, preds, A_test).minimum\n",
    "        disparity[i] = group_mean_prediction(y_test, preds, A_test).range\n",
    "        \n",
    "    plt.scatter(roc_auc, disparity)\n",
    "    plt.xlabel(\"Minimum ROC AUC score\")\n",
    "    plt.ylabel(\"Disparity in Opportunity\")\n",
    "    plt.show()\n",
    "    \n",
    "auc_roc_disparity_sweep_plot(first_multipliers, sweep.all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we need $\\lambda \\approx 6$, so expand a grid around there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_multipliers = np.linspace(5, 7, 21)\n",
    "\n",
    "second_sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid=grid_generator(second_multipliers))\n",
    "\n",
    "second_sweep.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the opportunity as a function of $\\lambda$ and the tradeoff between the ROC-AUC score and disparity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunity_sweep_plot(second_multipliers, second_sweep.all_results)\n",
    "auc_roc_disparity_sweep_plot(second_multipliers, second_sweep.all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Grid Search results\n",
    "\n",
    "We have used the `GridSearch` algorithm with demographic parity constraints. However, as noted above, for analysing our results, it is more appropriate to look at the predicted probabilities and the ROC-AUC scores.\n",
    "\n",
    "We can plot these scores against the mean disparity (between blacks and whites) for each model. We can see that for very little change in the worst ROC-AUC score, we can substantially reduce the disparity, as measured by the mean prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_roc_auc_score = np.zeros(n_second_sweep)\n",
    "sweep_mean_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(X_test)[:,1]\n",
    "    sweep_roc_auc_score[i] = group_roc_auc_score(y_test, preds, A_test).minimum\n",
    "    sweep_mean_disparity[i] = group_mean_prediction(y_test, preds, A_test).range\n",
    "    \n",
    "plt.scatter(sweep_roc_auc_score, sweep_mean_disparity)\n",
    "plt.xlabel(\"Minimum ROC AUC Score\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how the predictions are varying with the set of generated Lagrange multipliers. What we see is that we are gradually moving to predict that all students pass the bar. The 'opportunity gap' between the two sets of points matches the range in disparities in the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = [group_mean_prediction(y_test, x.predictor.predict_proba(X_test)[:,1], A_test)\n",
    "                   for x in second_sweep.all_results]\n",
    "\n",
    "for r in ['black', 'white']:\n",
    "    plt.scatter(second_sweep_multipliers, [x.by_group[r] for x in mean_predictions], label=r)\n",
    "plt.xlabel(\"Multiplier\")\n",
    "plt.ylabel(\"Opportunity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_mult = 30\n",
    "print(second_sweep_multipliers[i_mult])\n",
    "gs_preds = pd.Series(second_sweep.all_results[i_mult].predictor.predict_proba(X_test)[:,1], name=\"Predict_Proba\")\n",
    "group_metric_printer(\"ROC-AUC\", group_roc_auc_score(y_test, gs_preds, A_test))\n",
    "print(\"mean_prediction_disparity\", group_mean_prediction(y_test, gs_preds, A_test).range)\n",
    "plot_separated_cdf(gs_preds, A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation with Threshold Optimisation\n",
    "\n",
    "We can also use the post-processing approach from `fairlearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "class LogisticRegressionAsRegression:\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.logistic_regression_estimator.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator.predict_proba(X)[:,1]\n",
    "        return scores\n",
    "\n",
    "est = LogisticRegressionAsRegression(LogisticRegression(solver='liblinear', fit_intercept=True))\n",
    "\n",
    "postprocess_estimator = ThresholdOptimizer(estimator=est,\n",
    "                                          constraints=\"demographic_parity\")\n",
    "\n",
    "postprocess_estimator._plot = True\n",
    "postprocess_estimator.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_preds = postprocess_estimator.predict(X_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mean_predictions = group_mean_prediction(y_test, # Actually unused\n",
    "                                            pp_preds,\n",
    "                                            A_test)\n",
    "group_metric_printer(\"Predicted Pass Rate\", pp_mean_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(pp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_roc_auc_score = group_roc_auc_score(y_test, pp_preds, A_test)\n",
    "\n",
    "group_metric_printer(\"PP ROC-AUC\", pp_roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in ['black', 'white']:\n",
    "    Ys = y_train[A_train==r]\n",
    "    print(r)\n",
    "    print(Ys.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
