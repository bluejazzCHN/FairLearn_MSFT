{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Unfairness in the Law School Dataset\n",
    "\n",
    "In this example, we will examine the well known Law School Admissions dataset, provided by [Project SEAPHE](http://www.seaphe.org/databases.php). The motivation was to gain a better understanding of race in law school admissions, and ensuring that students who would ultimately pass the bar exam were treated fairly.\n",
    "\n",
    "We shall train a model for an admissions scenario. The model's task will be to provide a means of ranking applicants, and offers would be made to the top students from that list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We obtain the data from the `tempeh` package. The main feature data (which we will refer to as $X$) has two features - undergraduate GPA and LSAT score. The label (which we call $y$) is 0 or 1 dependent on whether that student passed the bar exam. Finally, we also have the race of the students ('black' or white') as a sensitive attribute, which we will refer to as $A$.\n",
    "\n",
    "We start by loading the data, which have already been split into \"train\" and \"test\" subsets for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "X_train = pd.DataFrame(dataset.X_train, columns=dataset.features)\n",
    "X_test = pd.DataFrame(dataset.X_test, columns=dataset.features)\n",
    "\n",
    "y_train = pd.Series(dataset.y_train.squeeze(), name=\"Pass Bar\", dtype=int)\n",
    "y_test = pd.Series(dataset.y_test.squeeze(), name=\"Pass Bar\", dtype=int)\n",
    "\n",
    "A_train = pd.Series(dataset.race_train, name=\"Race\")\n",
    "A_test = pd.Series(dataset.race_test, name=\"Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us examine the data. First, we can look at the breakdown of students by race in the dataset. We see that there are far more white students than black, which is already a suggestion of bias in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, c = np.unique(dataset.race_train, return_counts=True)\n",
    "for i in range(len(l)):\n",
    "    print(\"Number of {0} students is {1}\".format(l[i], c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start using the group metrics from `fairlearn` to examine things such as the final pass rate for the bar exam. Both rates are high, although higher for whites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_mean_prediction\n",
    "\n",
    "def group_metric_printer(name, group_metric_result):\n",
    "    print(\"{0} overall {1:.3f}\".format(name, group_metric_result.overall))\n",
    "    for k, v in group_metric_result.by_group.items():\n",
    "        print(\"{0} for {1:8} {2:.3f}\".format(name, k, v))\n",
    "\n",
    "unused = np.ones(len(dataset.y_train))\n",
    "group_metric_printer(\"Pass Rate\", group_mean_prediction(unused, y_train, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw numbers, we see how dominant whites who pass the bar exam are in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in ['black', 'white']:\n",
    "    Ys = y_train[A_train==r]\n",
    "    print(r)\n",
    "    print(Ys.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the [ROC-AUC scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for each feature (that is LSAT score and undergraduate GPA) for the overall dataset and by race. Used in this way, the ROC-AUC score is a measure of how predictive each feature is of the final label. A score of 0.5 would mean that the feature is no better than a coin flip (a coin biased to produce a desired fraction of positives), while a score of 1 means that the feature is perfectly discriminating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score\n",
    "\n",
    "for column_name in X_train:\n",
    "    column_data = X_train[column_name]\n",
    "    title = \"ROC-AUC {0}\".format(column_name)\n",
    "    group_metric_printer(title, group_roc_auc_score(y_train, column_data, A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the CDFs for the LSAT and GPAs for whites and blacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import cumfreq\n",
    "\n",
    "def plot_separated_cdf(data, A):\n",
    "    for a in np.unique(A):\n",
    "        subset = data[A==a]\n",
    "        \n",
    "        cdf = cumfreq(subset, numbins=20)\n",
    "        x = cdf.lowerlimit + np.linspace(0, cdf.binsize*cdf.cumcount.size, cdf.cumcount.size)\n",
    "        plt.plot(x, cdf.cumcount / len(subset), label=a)\n",
    "    plt.xlabel(data.name)\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "plot_separated_cdf(X_train['lsat'], A_train)\n",
    "plot_separated_cdf(X_train['ugpa'], A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unmitigated Predictor\n",
    "\n",
    "As a point of comparison for later, we can train a predictor without regard to fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine some statistics for this predictor. Since we have an admissions scenario, with a goal of ranking applicants, it is not useful to look at the binary prediction itself (especially since we already know that most students will pass the bar exam). Instead, we use the `predict_proba` method of the `LogisticRegression` estimator, which provides probabilities for each class. We have a binary label column, so we will focus on the \"1\" class, and we shall refer to the value as a 'score' (to be used in ranking) rather than a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_scores = pd.Series(unmitigated_predictor.predict_proba(X_test)[:,1], name=\"Unmitigated Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the mean predicted score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric_printer(\"Predicted Score\", group_mean_prediction(y_test, unmitigated_scores, A_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the ROC-AUC scores for the predictions. Due to the huge imbalance in the input population, this is a good metric to use in place of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_roc_auc_score_unmitigated = group_roc_auc_score(y_test, unmitigated_scores, A_test)\n",
    "group_metric_printer(\"Unmitigated ROC-AUC score\", group_roc_auc_score_unmitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can examine the distribution in predicted score in more detail. Also marked is the maximum distance between the two cumulative frequency curves. This is an alternative measure of disparity in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cdfs(data, A, num_bins=100):\n",
    "    cdfs = {}\n",
    "    assert len(np.unique(A)) == 2\n",
    "    \n",
    "    limits = ( min(data), max(data) )\n",
    "    s = 0.5 * (limits[1] - limits[0]) / (num_bins - 1)\n",
    "    limits = ( limits[0]-s, limits[1] + s)\n",
    "    \n",
    "    for a in np.unique(A):\n",
    "        subset = data[A==a]\n",
    "        \n",
    "        cdfs[a] = cumfreq(subset, numbins=num_bins, defaultreallimits=limits)\n",
    "        \n",
    "    lower_limits = [v.lowerlimit for _, v in cdfs.items()]\n",
    "    bin_sizes = [v.binsize for _,v in cdfs.items()]\n",
    "    actual_num_bins = [v.cumcount.size for _,v in cdfs.items()]\n",
    "    \n",
    "    assert len(np.unique(lower_limits)) == 1\n",
    "    assert len(np.unique(bin_sizes)) == 1\n",
    "    assert np.all([num_bins==v.cumcount.size for _,v in cdfs.items()])\n",
    "    \n",
    "    xs = lower_limits[0] + np.linspace(0, bin_sizes[0]*num_bins, num_bins)\n",
    "    \n",
    "    disparities = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        cdf_values = np.clip([v.cumcount[i]/len(data[A==k]) for k,v in cdfs.items()],0,1)\n",
    "        disparities[i] = max(cdf_values)-min(cdf_values)  \n",
    "    \n",
    "    return xs, cdfs, disparities\n",
    "    \n",
    "    \n",
    "def plot_and_compare_cdfs(data, A, num_bins=100):\n",
    "    xs, cdfs, disparities = compare_cdfs(data, A, num_bins)\n",
    "    \n",
    "    for k, v in cdfs.items():\n",
    "        plt.plot(xs, v.cumcount/len(data[A==k]), label=k)\n",
    "    \n",
    "    assert disparities.argmax().size == 1\n",
    "    d_idx = disparities.argmax()\n",
    "    \n",
    "    xs_line = [xs[d_idx],xs[d_idx]]\n",
    "    counts = [v.cumcount[d_idx]/len(data[A==k]) for k, v in cdfs.items()]\n",
    "    ys_line = [min(counts), max(counts)]\n",
    "    \n",
    "    plt.plot(xs_line, ys_line, 'o--')\n",
    "    disparity_label = \"Max Disparity = {0:.3f} \\nat {1:0.3f} \".format(disparities[d_idx], xs[d_idx])\n",
    "    plt.text(xs[d_idx], 1, disparity_label, ha=\"right\", va=\"top\")\n",
    "    \n",
    "    plt.xlabel(data.name)\n",
    "    plt.ylabel(\"Cumulative Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_and_compare_cdfs(unmitigated_scores, A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfairness Mitigation with Grid Search\n",
    "\n",
    "In this section, we will attempt to mitigate the unfairness in the incoming data using the `GridSearch` algorithm of `fairlearn`. We shall apply constraints of demographic parity - that is, we will attempt to equalise the positive prediction rates between whites and blacks. This is appropriate for affirmative action scenarios.\n",
    "\n",
    "We will compute 41 models, on a grid covering the range $[-10, 10]$. The following cell may take a couple of minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, DemographicParity\n",
    "\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=41,\n",
    "                   grid_limit=10)\n",
    "\n",
    "sweep.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the mean opportunity (given by the mean prediction in this case) of these models as a function of the multiplier used. We can also see that the opportunity is equalised for blacks and whites with a multiplier of around six."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_sweep_plot(all_results, metric_func):\n",
    "    xs = range(len(all_results))\n",
    "    metrics = [metric_func(y_test, x.predictor.predict_proba(X_test)[:,1], A_test)\n",
    "               for x in all_results]\n",
    "    \n",
    "    for r in ['black', 'white']:\n",
    "        plt.plot(xs, [x.by_group[r] for x in metrics], label=r)\n",
    "    plt.plot(xs, [x.overall for x in metrics], label='overall')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(metric_func.__name__[6:])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "metric_sweep_plot(sweep.all_results, metric_func=group_mean_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the ROC-AUC score for this set of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_sweep_plot(sweep.all_results, metric_func=group_roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the minimum of the ROC-AUC score against the disparity in opportunity for each model in the sweep. This gives us an overview of the tradeoffs available to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_disparity_sweep_plot(all_results):\n",
    "    roc_auc = np.zeros(len(all_results))\n",
    "    disparity = np.zeros(len(all_results))\n",
    "    \n",
    "    for i in range(len(all_results)):\n",
    "        preds = all_results[i].predictor.predict_proba(X_test)[:,1]\n",
    "        roc_auc[i] = group_roc_auc_score(y_test, preds, A_test).minimum\n",
    "        disparity[i] = group_mean_prediction(y_test, preds, A_test).range\n",
    "        \n",
    "    plt.scatter(roc_auc, disparity)\n",
    "    plt.xlabel(\"Minimum ROC AUC score\")\n",
    "    plt.ylabel(\"Disparity in Opportunity\")\n",
    "    plt.show()\n",
    "    print(\"Index of minimum disparity\", disparity.argmin())\n",
    "    \n",
    "roc_auc_disparity_sweep_plot(sweep.all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to looking at the disparity in opportunity, we can use the maximum distance between the score CDFs (for blacks and whites) as the disparity metric for each model. This gives the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_cdf_disparity_sweep_plot(all_results):\n",
    "    roc_auc = np.zeros(len(all_results))\n",
    "    disparity = np.zeros(len(all_results))\n",
    "    \n",
    "    for i in range(len(all_results)):\n",
    "        preds = all_results[i].predictor.predict_proba(X_test)[:,1]\n",
    "        roc_auc[i] = group_roc_auc_score(y_test, preds, A_test).minimum\n",
    "        _, _, dis = compare_cdfs(preds, A_test)\n",
    "        disparity[i] = dis.max()\n",
    "        \n",
    "    plt.scatter(roc_auc, disparity)\n",
    "    plt.xlabel(\"Minimum ROC AUC score\")\n",
    "    plt.ylabel(\"Disparity from CDF\")\n",
    "    plt.show()\n",
    "    print(\"Index of minimum Disparity \", disparity.argmin())\n",
    "    \n",
    "roc_auc_cdf_disparity_sweep_plot(sweep.all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at several different models, identified by their index in the results from the grid search. One relevant one is obviously the one with minimum disparity, which occurs at index 33, regardless of the disparity metric chosen. However, the ROC-AUC score is indicating that we're barely better than chosing at random here (slightly better than random for whites, worse than random for blacks - in such a case, one would flip the prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_and_cdf(predictor):\n",
    "    scores = pd.Series(predictor.predict_proba(X_test)[:,1], name=\"Scores\")\n",
    "    \n",
    "    group_metric_printer(\"Chosen ROC-AUC score\", group_roc_auc_score(y_test, scores, A_test))\n",
    "    print(\"Disparity in Opportunity {0:.3f}\".format(group_mean_prediction(y_test, scores, A_test).range))\n",
    "    \n",
    "    plot_and_compare_cdfs(scores, A_test)\n",
    "\n",
    "roc_auc_and_cdf(sweep.all_results[33].predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can substantially increase the ROC-AUC score by accepting a slightly higher disparity. The change in disparity in opportunity is minimal, while the change based on the cumulative frequencies is rather larger. However, compared to the unmitigated model above, this is still a substantial improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_and_cdf(sweep.all_results[30].predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation with Threshold Optimisation\n",
    "\n",
    "We can also use the post-processing approach from `fairlearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "class LogisticRegressionAsRegression:\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.logistic_regression_estimator.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator.predict_proba(X)[:,1]\n",
    "        return scores\n",
    "\n",
    "est = LogisticRegressionAsRegression(LogisticRegression(solver='liblinear', fit_intercept=True))\n",
    "\n",
    "postprocess_estimator = ThresholdOptimizer(estimator=est,\n",
    "                                          constraints=\"demographic_parity\")\n",
    "\n",
    "postprocess_estimator._plot = True\n",
    "postprocess_estimator.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_preds = postprocess_estimator.predict(X_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mean_predictions = group_mean_prediction(y_test, # Actually unused\n",
    "                                            pp_preds,\n",
    "                                            A_test)\n",
    "group_metric_printer(\"Predicted Pass Rate\", pp_mean_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(pp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_roc_auc_score = group_roc_auc_score(y_test, pp_preds, A_test)\n",
    "\n",
    "group_metric_printer(\"PP ROC-AUC\", pp_roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
