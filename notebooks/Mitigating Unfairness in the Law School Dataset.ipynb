{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Unfairness in the Law School Dataset\n",
    "\n",
    "In this example, we will examine the well known Law School Admissions dataset, provided by [Project SEAPHE](http://www.seaphe.org/databases.php). The motivation was to gain a better understanding of race in law school admissions, and ensuring that students who would ultimately pass the bar exam were treated fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We obtain the data from the `tempeh` package. The main feature data (which we will refer to as $X$) has two features - undergraduate GPA and LSAT score. The label (which we call $y$) is 0 or 1 dependent on whether that student passed the bar exam. Finally, we also have the race of the students ('black' or white') as a sensitive attribute, which we will refer to as $A$.\n",
    "\n",
    "We start by loading the data, which have already been split into \"train\" and \"test\" subsets for us. However, we do need to rescale the two features in $X$ to lie in the range $[0, 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(dataset.X_train), columns=dataset.features)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(dataset.X_test), columns=dataset.features)\n",
    "\n",
    "y_train = pd.Series(dataset.y_train.squeeze(), name=\"Pass Bar\")\n",
    "y_test = pd.Series(dataset.y_test.squeeze(), name=\"Pass Bar\")\n",
    "\n",
    "A_train = pd.Series(dataset.race_train, name=\"Race\")\n",
    "A_test = pd.Series(dataset.race_test, name=\"Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us examine the data. First, we can look at the breakdown of students by race in the dataset. We see that there are far more white students than black, which is already a suggestion of bias in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, c = np.unique(dataset.race_train, return_counts=True)\n",
    "for i in range(len(l)):\n",
    "    print(\"Number of {0} students is {1}\".format(l[i], c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start using the group metrics from `fairlearn` to examine things such as the final pass rate for the bar exam. Both rates are high, although higher for whites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_mean_prediction\n",
    "\n",
    "def group_metric_printer(name, group_metric_result):\n",
    "    print(\"{0} overall {1:.3f}\".format(name, group_metric_result.overall))\n",
    "    for k, v in group_metric_result.by_group.items():\n",
    "        print(\"{0} for {1:8} {2:.3f}\".format(name, k, v))\n",
    "\n",
    "unused = np.ones(len(dataset.y_train))\n",
    "group_metric_printer(\"Pass Rate\", group_mean_prediction(unused, y_train, A_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score\n",
    "\n",
    "for column_name in X_train:\n",
    "    column_data = X_train[column_name]\n",
    "    title = \"ROC-AUC {0}\".format(column_name)\n",
    "    group_metric_printer(title, group_roc_auc_score(y_train, column_data, A_train))\n",
    "\n",
    "#dataset.X_train[:,0]\n",
    "\n",
    "#print(dataset.features)\n",
    "\n",
    "# Should look at these by group too\n",
    "#print(roc_auc_score(dataset.y_test, dataset.X_test[:,0]))\n",
    "#print(roc_auc_score(dataset.y_test, dataset.X_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unmitigated Predictor\n",
    "\n",
    "As a point of comparison for later, we can train a predictor without regard to fairness. Recall that the training features in `X` are just LSAT and GPA scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(dataset.X_train, dataset.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this predictor, we can look at some statistics. First, we examine the normal error and disparity metrics one might expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unmitigated = unmitigated_predictor.predict_proba(dataset.X_test)[:,1]\n",
    "print(y_pred_unmitigated)\n",
    "\n",
    "from fairlearn.metrics import group_zero_one_loss, group_mean_prediction\n",
    "\n",
    "# unmitigated_zero_one_loss = group_zero_one_loss(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "unmitigated_mean_prediction = group_mean_prediction(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "\n",
    "#print(\"Zero One Loss Overall {0:.3f}\".format(unmitigated_zero_one_loss.overall))\n",
    "#print(\"Zero One Loss whites  {0:.3f}\".format(unmitigated_zero_one_loss.by_group['white']))\n",
    "#print(\"Zero One Loss blacks  {0:.3f}\".format(unmitigated_zero_one_loss.by_group['black']))\n",
    "print()\n",
    "print(\"Mean Prediction Overall {0:.3f}\".format(unmitigated_mean_prediction.overall))\n",
    "print(\"Mean Prediction whites  {0:.3f}\".format(unmitigated_mean_prediction.by_group['white']))\n",
    "print(\"Mean Prediction blacks  {0:.3f}\".format(unmitigated_mean_prediction.by_group['black']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high general pass rate is obviously causing problems, since this predictor is giving a 100% pass rate for whites and only causing a small error as a result. The disparity (the difference in the mean prediction for blacks and whites) is also very small for similar reasons.\n",
    "\n",
    "With such unbalanced classes, it is better to use `group_balanced_root_mean_squared_error` to track disparity, since this puts equal weight on over- and under-predictions, regardless of the relative number of either. Similarly, the error is best tracked via the `group_roc_auc_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score, group_balanced_root_mean_squared_error\n",
    "\n",
    "print(roc_auc_score(dataset.y_test, y_pred_unmitigated))\n",
    "print(unmitigated_predictor.coef_)\n",
    "print(y_pred_unmitigated)\n",
    "\n",
    "unmitigated_group_roc_auc_score = group_roc_auc_score(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "print(\"roc white\", unmitigated_group_roc_auc_score.by_group['white'])\n",
    "print(\"roc black\", unmitigated_group_roc_auc_score.by_group['black'])\n",
    "print(\"Unmitigated Error {0:.3f}\".format(1 - unmitigated_group_roc_auc_score.maximum))\n",
    "\n",
    "unmitigated_group_balanced_rms_error = group_balanced_root_mean_squared_error(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "print(\"Unmitigated Disparity {0:.3f}\".format(unmitigated_group_balanced_rms_error.range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing a Grid Search\n",
    "\n",
    "We do a grid search in two stages. In the first, we do a low resolution search, with `fairlearn` chosing the grid for us. From this, we identify a region to expand the grid, and do a more detailed sweep around that point.\n",
    "\n",
    "First the low resolution sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sweep = 9\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=n_sweep)\n",
    "\n",
    "sweep.fit(dataset.X_train, dataset.y_train, sensitive_features=dataset.race_train)\n",
    "\n",
    "print(sweep.best_result.lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best $\\lambda$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec_best = sweep.best_result.lambda_vec\n",
    "lambda_best = lambda_vec_best[(\"+\", \"all\", \"white\")] - lambda_vec_best[(\"-\", \"all\", \"white\")]\n",
    "print(\"lambda_best =\", lambda_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a higher resolution grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_second_sweep = 51\n",
    "second_sweep_multipliers = np.linspace(lambda_best-0.5, lambda_best+0.5, n_second_sweep)\n",
    "\n",
    "iterables = [['+','-'], ['all'], ['black', 'white']]\n",
    "midx = pd.MultiIndex.from_product(iterables, names=['sign', 'event', 'group_id'])\n",
    "\n",
    "second_sweep_lambdas = []\n",
    "for l in second_sweep_multipliers:\n",
    "    nxt = pd.Series(np.zeros(4), index=midx)\n",
    "    if l < 0:\n",
    "        nxt[(\"-\", \"all\", \"white\")] = abs(l)\n",
    "    else:\n",
    "        nxt[(\"+\", \"all\", \"white\")] = l\n",
    "    second_sweep_lambdas.append(nxt)\n",
    "    \n",
    "multiplier_df = pd.concat(second_sweep_lambdas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the new search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid=multiplier_df)\n",
    "\n",
    "second_sweep.fit(dataset.X_train, dataset.y_train, sensitive_features=dataset.race_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Analysis\n",
    "\n",
    "We can look at our results using the zero-one loss and mean prediction metrics. These are not very useful, as we shall see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_naive_error = np.zeros(n_second_sweep)\n",
    "sweep_naive_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(dataset.X_test)[:,1]\n",
    "    sweep_naive_error[i] = group_roc_auc_score(dataset.y_test, preds, dataset.race_test).maximum\n",
    "    sweep_naive_disparity[i] = group_mean_prediction(dataset.y_test, preds, dataset.race_test).range\n",
    "    \n",
    "plt.scatter(sweep_naive_error, sweep_naive_disparity)\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Alternative Metrics\n",
    "\n",
    "As noted above, it is better to use the `group_roc_auc_score` metric for the error, and the `group_balanced_root_mean_squared_error` for disparity. With these metrics, we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_error = np.zeros(n_second_sweep)\n",
    "sweep_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(dataset.X_test)[:,1]\n",
    "    sweep_error[i] = group_roc_auc_score(dataset.y_test, preds, dataset.race_test).overall\n",
    "    sweep_disparity[i] = group_balanced_root_mean_squared_error(dataset.y_test, preds, dataset.race_test).range\n",
    "    \n",
    "plt.scatter(sweep_error, sweep_disparity)\n",
    "plt.xlabel(\"ROC AUC Score\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how the predictions are varying with the set of generated Lagrange multipliers. What we see is that we are gradually moving to predict that all students pass the bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = [group_mean_prediction(dataset.y_test, x.predictor.predict_proba(dataset.X_test)[:,1], dataset.race_test)\n",
    "                   for x in second_sweep.all_results]\n",
    "\n",
    "for r in ['black', 'white']:\n",
    "    plt.scatter(second_sweep_multipliers, [x.by_group[r] for x in mean_predictions], label=r)\n",
    "plt.xlabel(\"Multiplier\")\n",
    "plt.ylabel(\"Opportunity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
