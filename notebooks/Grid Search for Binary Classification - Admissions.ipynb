{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Binary Classification - Admissions\n",
    "\n",
    "In this notebook, we will apply the `GridSearch` algorithm in FairLearn to a binary classification problem, where we also have a binary protected attribute. This algorithm comes from the paper [\"A Reductions Approach to Fair Classification\" (Agarwal et al. 2018)](https://arxiv.org/abs/1803.02453). The grid search is a simplified version of the full algorithm (appearing in section 3.4), which works best for binary classification and a binary protected attribute.\n",
    "\n",
    "The specific problem we consider is a biased college admissions problem. We assume that we have a group of males and females (gender will be our protected attribute), with matching standardised test scores and some other irrelevant feature which is correlated with gender. We also have a set of labels denoting whether or not each individual was admitted, and we will make this (generated) historical data biased, by setting a higher threshold for females than males. We will make the standardised test scores independent of gender, so if admissions were unbiased, both genders would be admitted in equal portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data\n",
    "\n",
    "We are going to synthesise data for this scenario. We will generate a dataset with two features - \"score\" and \"irrelevant.\" Both will be follow a normal distribution, but while the \"score\" feature will be parameterised by a single mean and standard deviation, the \"irrelevant\" feature will be affected by gender. Similarly in the biased historical data, we will set different thresholds for the two genders (with a small around of normally distributed jitter around each threshold).\n",
    "\n",
    "The following class implements this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self):\n",
    "        # We use label and index 0 for female and 1 for male\n",
    "        self.number = [100, 300]\n",
    "        \n",
    "        self.score_mean = 0.5\n",
    "        self.score_std_dev = 0.1\n",
    "        \n",
    "        self.score_threshold = [0.6, 0.4]\n",
    "        self.score_threshold_jitter = [0.05, 0.05]\n",
    "        \n",
    "        self.irrelevant_mean = [0.3, 0.7]\n",
    "        self.irrelevant_std_dev = [0.1, 0.1]\n",
    "        \n",
    "    def generate(self):\n",
    "        genders = []\n",
    "        scores = []\n",
    "        admissions = []\n",
    "        irrelevants = []\n",
    "\n",
    "        for g in range(2):\n",
    "            s, a, ir = self._generate_single_dataset(self.number[g],\n",
    "                                                     self.score_threshold[g],\n",
    "                                                     self.score_threshold_jitter[g],\n",
    "                                                     self.irrelevant_mean[g],\n",
    "                                                     self.irrelevant_std_dev[g])\n",
    "            genders.append(np.full(self.number[g], g))\n",
    "            scores.append(s)\n",
    "            admissions.append(a)\n",
    "            irrelevants.append(ir)\n",
    "        \n",
    "        all_scores = np.concatenate( (scores[0], scores[1]), axis=None)\n",
    "        all_admissions = np.concatenate( (admissions[0], admissions[1]), axis=None)\n",
    "        all_irrelevants = np.concatenate( (irrelevants[0], irrelevants[1]), axis=None)\n",
    "        all_genders = np.concatenate( (genders[0], genders[1]), axis=None)\n",
    "        \n",
    "        A = pd.Series(data=all_genders, name=\"Gender\")\n",
    "        X = pd.DataFrame({\"score\":all_scores,\n",
    "                          \"irrelevant\": all_irrelevants})\n",
    "        Y = pd.Series(data=all_admissions, name=\"Admitted\")\n",
    "        \n",
    "        return X, Y, A\n",
    "        \n",
    "    def _generate_single_dataset(self,\n",
    "                                 number_samples,\n",
    "                                 threshold, threshold_jitter,\n",
    "                                 irr_mean, irr_std_dev):\n",
    "        scores = np.random.normal(loc=self.score_mean,\n",
    "                                  scale=self.score_std_dev,\n",
    "                                  size=number_samples)\n",
    "        scores[ scores < 0 ] = 0\n",
    "        scores[ scores > 1 ] = 1\n",
    "    \n",
    "        threshold = np.random.normal(loc=threshold, scale=threshold_jitter, size=number_samples)\n",
    "        threshold[ threshold < 0 ] = 0\n",
    "        threshold[ threshold > 1 ] = 1\n",
    "    \n",
    "        def admit(s, t): return int(s > t)\n",
    "    \n",
    "        vadmit = np.vectorize(admit)\n",
    "    \n",
    "        admitted = vadmit(scores, threshold)\n",
    "    \n",
    "        irrelevant = np.random.normal(loc=irr_mean, scale=irr_std_dev, size=number_samples)\n",
    "    \n",
    "        return scores, admitted, irrelevant\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this class to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator()\n",
    "\n",
    "X, Y, A = dg.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `matplotlib` to examine some of the data. First we examine the distribution of the data in the `X` feature array. As expected, the \"score\" feature has an identical distribution, but the \"irrelevant\" feature shows a gender difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_width = 12\n",
    "plot_height = 8\n",
    "plt.rcParams[\"figure.figsize\"] = (plot_width, plot_height) # (w, h)\n",
    "\n",
    "# Nice caption text\n",
    "gender_labels = [\"Female\", \"Male\"]\n",
    "\n",
    "# Plot two histograms for the given column\n",
    "def histograms(X_s, A_s, col_name):\n",
    "    \n",
    "    sep_data = [X_s[col_name][A_s==0], X_s[col_name][A_s==1]]\n",
    "    \n",
    "    plt.hist(sep_data, histtype=\"step\", bins=20, label=gender_labels)\n",
    "    plt.xlabel(col_name)\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "histograms(X, A, \"score\")\n",
    "histograms(X, A, \"irrelevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine whether each individual was admitted as a function of their test score. This clearly shows the bias against females:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_admissions_vs_scores(X_s, Y_s, A_s):\n",
    "    markers=[\".\", \"x\"]\n",
    "    for i in range(2):\n",
    "        mask = A_s == i\n",
    "        plt.scatter(X_s[mask].score, Y_s[mask], label=gender_labels[i], marker=markers[i])\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Admitted\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_admissions_vs_scores(X, Y, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the above data in a slightly different way - as the fraction of each gender admitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_admission_fractions(Y_s, A_s):\n",
    "    f_frac = np.mean(Y_s[A_s==0])\n",
    "    m_frac = np.mean(Y_s[A_s==1])\n",
    "    \n",
    "    plt.bar(x=range(2), height=[f_frac, m_frac], tick_label=gender_labels)\n",
    "    plt.ylabel(\"Admisison Fraction\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_admission_fractions(Y, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Unmitigated Model\n",
    "\n",
    "Before we try mitigating the bias, we can first try training a naive model. For simplicity, we will use the `LogisticRegression` estimator from `sklearn`. Once the model is trained, we can examine the values of the `coef_` array which stores the coefficients of each column from the model - the first corresponds to the \"score\" feature, while the second corresponds to the \"irrelevant\" feature. We can see that there is a non-zero weight on the \"irrelevant\" feature. In fact, it has almost as much weight as the \"score\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_model = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "unmitigated_model.fit(X, Y)\n",
    "\n",
    "unmitigated_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain predictions from this model, and examine them. It's obvious that the bias has been maintained in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_unmitigated = unmitigated_model.predict(X)\n",
    "\n",
    "plot_admissions_vs_scores(X, Y_predict_unmitigated, A)\n",
    "plot_admission_fractions(Y_predict_unmitigated, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Unfairness with Grid Search\n",
    "\n",
    "Now, we move on to attempting to reduce the unfairness in our model using the grid search. This tries a series of different models, parameterised by a Lagrange multiplier $\\lambda_i$. For each value of $\\lambda$, the algorithm reweights and relabels the input data, and trains a fresh model ($\\lambda=0$ corresponds to the unaltered case).\n",
    "\n",
    "The grid search acts like a normal `sklearn` estimator, implementing `fit()` and `predict()` methods. The `fit()` method performs the grid search, and the best model found (according to a specified selection rule) is used in `predict()` calls. However, after `fit()` is called, there are two extra properties on the estimator - a `best_result` and a list `all_results`; the `best_result` is used by `predict()` while `all_results` corresponds to the output of the grid search itself. The items in each are dictionaries, each with four entries - `lambda_vec`, `objective`, `gamma` and `predictor`.\n",
    "\n",
    "We start by telling the algorithm that we want to try 7 different values of $\\lambda$ (which are generated for us). We specify demographic parity as our constraint - since the distribution of scores is the same for both genders, we should expect that the admission fraction for each should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sweep=GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                       constraints=DemographicParity(),\n",
    "                       grid_size=7)\n",
    "\n",
    "first_sweep.fit(X, Y, sensitive_features=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the values of $\\lambda_i$ chosen for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vecs = [x.lambda_vec for x in (first_sweep.all_results)]\n",
    "lambda_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is rather more than a single value $\\lambda$, so what's going on? These results are the outputs of the `Moment` type which drives the reduction approach to removing disparity. There are actually four Lagrange multipliers here, indexed by a tuple (sign, event, group_id). The 'group_id' field corresponds to the labels 0 (for \"female\") and 1 (for \"male\", while the 'grp' field is the same in all cases (this is because we have specified Demographic Parity as our disparity criterion). Finally the 'sign' comes from the reductions approach specifying separate multipliers for violations of the disparity criterion from above and below. Both of these are constrained to be positive.\n",
    "\n",
    "So we have four multipliers - $\\lambda_{(+,0)}$, $\\lambda_{(-,0)}$, $\\lambda_{(+,1)}$ and $\\lambda_{(-,1)}$. Without losing generality, we can decide to modify one of these, but not the other, and the `DemographicParity` object we passed to the `GridSearch` constructor chose to make $\\lambda_{(+,1)}=\\lambda_{(-,1)}=0$. Finally, we can combine the 'above' and 'below' multipliers for the other group and obtain $\\lambda_i = \\lambda_{(+,0)} - \\lambda_{(-,0)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_multipliers = [x[(\"+\", \"all\", 0)]-x[(\"-\", \"all\", 0)] for x in lambda_vecs]\n",
    "actual_multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at how the weight the models place on the protected attribute (recall that in the fair case, this would be zero) varies with $\\lambda_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sweep_irrelevant_weights = [\n",
    "    x.predictor.coef_[0][1] for x in first_sweep.all_results\n",
    "]\n",
    "\n",
    "plt.scatter(actual_multipliers, first_sweep_irrelevant_weights)\n",
    "plt.xlabel(\"Lagrange Multiplier\")\n",
    "plt.ylabel(\"Weight on irrelevant feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
