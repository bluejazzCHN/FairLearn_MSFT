{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search on the Law School Dataset\n",
    "\n",
    "In this example, we look at the well known Law School dataset. We use the preprocessed version of this from `tempeh` which has two feature columns (the normalized LSAT and GPA for each person), a sensitive attribute of race (with values 'white' and 'black') and a label column indicating whether that student passed the bar exam. We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity\n",
    "\n",
    "from fairlearn.metrics import group_zero_one_loss, group_mean_prediction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "# Sort out some dimensions\n",
    "dataset.y_train = dataset.y_train.squeeze()\n",
    "dataset.y_test = dataset.y_test.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a quick look at the data, in particular the number of students who were black and white, and the label column. For the first, we see that the number of white students is more than an order of magnitude larger. Furthermore, almost all students pass the bar exam (although the pass rate for whites is higher):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, c = np.unique(dataset.race_train, return_counts=True)\n",
    "for i in range(len(l)):\n",
    "    print(\"Number {0} is {1}\".format(l[i], c[i]))\n",
    "\n",
    "unused = np.ones(len(dataset.y_train))\n",
    "mean_prediction = group_mean_prediction(unused, dataset.y_train, dataset.race_train)\n",
    "print(\"Overall pass rate {0:.3f}\".format(mean_prediction.overall))\n",
    "print(\"White pass rate   {0:.3f}\".format(mean_prediction.by_group['white']))\n",
    "print(\"Black pass rate   {0:.3f}\".format(mean_prediction.by_group['black']))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dataset.X_train[:,0]\n",
    "\n",
    "print(dataset.features)\n",
    "\n",
    "# Should look at these by group too\n",
    "print(roc_auc_score(dataset.y_test, dataset.X_test[:,0]))\n",
    "print(roc_auc_score(dataset.y_test, dataset.X_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unmitigated Predictor\n",
    "\n",
    "As a point of comparison for later, we can train a predictor without regard to fairness. Recall that the training features in `X` are just LSAT and GPA scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(dataset.X_train, dataset.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this predictor, we can look at some statistics. First, we examine the normal error and disparity metrics one might expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unmitigated = unmitigated_predictor.predict_proba(dataset.X_test)[:,1]\n",
    "print(y_pred_unmitigated)\n",
    "\n",
    "from fairlearn.metrics import group_zero_one_loss, group_mean_prediction\n",
    "\n",
    "# unmitigated_zero_one_loss = group_zero_one_loss(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "unmitigated_mean_prediction = group_mean_prediction(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "\n",
    "#print(\"Zero One Loss Overall {0:.3f}\".format(unmitigated_zero_one_loss.overall))\n",
    "#print(\"Zero One Loss whites  {0:.3f}\".format(unmitigated_zero_one_loss.by_group['white']))\n",
    "#print(\"Zero One Loss blacks  {0:.3f}\".format(unmitigated_zero_one_loss.by_group['black']))\n",
    "print()\n",
    "print(\"Mean Prediction Overall {0:.3f}\".format(unmitigated_mean_prediction.overall))\n",
    "print(\"Mean Prediction whites  {0:.3f}\".format(unmitigated_mean_prediction.by_group['white']))\n",
    "print(\"Mean Prediction blacks  {0:.3f}\".format(unmitigated_mean_prediction.by_group['black']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high general pass rate is obviously causing problems, since this predictor is giving a 100% pass rate for whites and only causing a small error as a result. The disparity (the difference in the mean prediction for blacks and whites) is also very small for similar reasons.\n",
    "\n",
    "With such unbalanced classes, it is better to use `group_balanced_root_mean_squared_error` to track disparity, since this puts equal weight on over- and under-predictions, regardless of the relative number of either. Similarly, the error is best tracked via the `group_roc_auc_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score, group_balanced_root_mean_squared_error\n",
    "\n",
    "print(roc_auc_score(dataset.y_test, y_pred_unmitigated))\n",
    "print(unmitigated_predictor.coef_)\n",
    "print(y_pred_unmitigated)\n",
    "\n",
    "unmitigated_group_roc_auc_score = group_roc_auc_score(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "print(\"roc white\", unmitigated_group_roc_auc_score.by_group['white'])\n",
    "print(\"roc black\", unmitigated_group_roc_auc_score.by_group['black'])\n",
    "print(\"Unmitigated Error {0:.3f}\".format(1 - unmitigated_group_roc_auc_score.maximum))\n",
    "\n",
    "unmitigated_group_balanced_rms_error = group_balanced_root_mean_squared_error(dataset.y_test, y_pred_unmitigated, dataset.race_test)\n",
    "print(\"Unmitigated Disparity {0:.3f}\".format(unmitigated_group_balanced_rms_error.range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing a Grid Search\n",
    "\n",
    "We do a grid search in two stages. In the first, we do a low resolution search, with `fairlearn` chosing the grid for us. From this, we identify a region to expand the grid, and do a more detailed sweep around that point.\n",
    "\n",
    "First the low resolution sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sweep = 9\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=n_sweep)\n",
    "\n",
    "sweep.fit(dataset.X_train, dataset.y_train, sensitive_features=dataset.race_train)\n",
    "\n",
    "print(sweep.best_result.lambda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best $\\lambda$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec_best = sweep.best_result.lambda_vec\n",
    "lambda_best = lambda_vec_best[(\"+\", \"all\", \"white\")] - lambda_vec_best[(\"-\", \"all\", \"white\")]\n",
    "print(\"lambda_best =\", lambda_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a higher resolution grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_second_sweep = 51\n",
    "second_sweep_multipliers = np.linspace(lambda_best-0.5, lambda_best+0.5, n_second_sweep)\n",
    "\n",
    "iterables = [['+','-'], ['all'], ['black', 'white']]\n",
    "midx = pd.MultiIndex.from_product(iterables, names=['sign', 'event', 'group_id'])\n",
    "\n",
    "second_sweep_lambdas = []\n",
    "for l in second_sweep_multipliers:\n",
    "    nxt = pd.Series(np.zeros(4), index=midx)\n",
    "    if l < 0:\n",
    "        nxt[(\"-\", \"all\", \"white\")] = abs(l)\n",
    "    else:\n",
    "        nxt[(\"+\", \"all\", \"white\")] = l\n",
    "    second_sweep_lambdas.append(nxt)\n",
    "    \n",
    "multiplier_df = pd.concat(second_sweep_lambdas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the new search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid=multiplier_df)\n",
    "\n",
    "second_sweep.fit(dataset.X_train, dataset.y_train, sensitive_features=dataset.race_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Analysis\n",
    "\n",
    "We can look at our results using the zero-one loss and mean prediction metrics. These are not very useful, as we shall see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_naive_error = np.zeros(n_second_sweep)\n",
    "sweep_naive_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(dataset.X_test)[:,1]\n",
    "    sweep_naive_error[i] = group_roc_auc_score(dataset.y_test, preds, dataset.race_test).maximum\n",
    "    sweep_naive_disparity[i] = group_mean_prediction(dataset.y_test, preds, dataset.race_test).range\n",
    "    \n",
    "plt.scatter(sweep_naive_error, sweep_naive_disparity)\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Alternative Metrics\n",
    "\n",
    "As noted above, it is better to use the `group_roc_auc_score` metric for the error, and the `group_balanced_root_mean_squared_error` for disparity. With these metrics, we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_error = np.zeros(n_second_sweep)\n",
    "sweep_disparity = np.zeros(n_second_sweep)\n",
    "\n",
    "for i in range(n_second_sweep):\n",
    "    preds = second_sweep.all_results[i].predictor.predict_proba(dataset.X_test)[:,1]\n",
    "    sweep_error[i] = group_roc_auc_score(dataset.y_test, preds, dataset.race_test).overall\n",
    "    sweep_disparity[i] = group_balanced_root_mean_squared_error(dataset.y_test, preds, dataset.race_test).range\n",
    "    \n",
    "plt.scatter(sweep_error, sweep_disparity)\n",
    "plt.xlabel(\"ROC AUC Score\")\n",
    "plt.ylabel(\"Disparity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how the predictions are varying with the set of generated Lagrange multipliers. What we see is that we are gradually moving to predict that all students pass the bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = [group_mean_prediction(dataset.y_test, x.predictor.predict_proba(dataset.X_test)[:,1], dataset.race_test)\n",
    "                   for x in second_sweep.all_results]\n",
    "\n",
    "for r in ['black', 'white']:\n",
    "    plt.scatter(second_sweep_multipliers, [x.by_group[r] for x in mean_predictions], label=r)\n",
    "plt.xlabel(\"Multiplier\")\n",
    "plt.ylabel(\"Opportunity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
